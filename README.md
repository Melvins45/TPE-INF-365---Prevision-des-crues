# Syst√®me Intelligent de Pr√©vision des Crues (TPE INF 365)
**Projet Acad√©mique - Groupe 24**

## üåä Vue d'ensemble du Projet
Ce syst√®me utilise le **Machine Learning** de pointe pour quantifier le risque d'inondation √† partir de 20 param√®tres critiques. Il a √©t√© con√ßu comme une solution d'aide √† la d√©cision pour les autorit√©s de protection civile et les urbanistes, permettant une anticipation proactive des catastrophes climatiques.

### üéØ Objectifs de ce Travail
1.  **Mod√©lisation Pr√©dictive** : Comparer et optimiser des algorithmes de r√©gression (Lin√©aire, For√™t Al√©atoire, R√©seaux de Neurones).
2.  **Analyse d'Impact** : Identifier les facteurs soci√©taux (d√©forestation, urbanisation) et m√©t√©orologiques aggravant le risque.
3.  **Interface Intuitive** : Offrir un outil de simulation interactive pour le grand public et les experts.

---

## üìä Donn√©es et M√©thodologie

### Le Dataset
- **Localisation** : [datas/flood.csv](datas/flood.csv)
- **Volume** : Plus de 50 000 exemples d'inondations historiques.
- **Param√®tres (20)** : Incluant l'intensit√© de la mousson, la qualit√© des infrastructures, la d√©forestation, et la stabilit√© politique.

### Les Mod√®les Impl√©ment√©s
1.  **For√™t Al√©atoire (Random Forest)** : Notre mod√®le le plus performant, capable de capturer des interactions non-lin√©aires complexes. (Voir [modeleForetAleatoire.ipynb](modeleForetAleatoire.ipynb)).
2.  **R√©gression Lin√©aire** : Mod√®le de r√©f√©rence pour sa simplicit√© et sa transparence. (Voir [modeleRegressionLineaire.ipynb](modeleRegressionLineaire.ipynb)).
3.  **R√©seaux de Neurones (MLP)** : Utilisation de Keras/TensorFlow pour explorer les relations profondes. (Voir [modeleReseauNeuronesSequential.ipynb](modeleReseauNeuronesSequential.ipynb)).

---

## üñ•Ô∏è Logiciel de Pr√©diction (GUI)

L'application principale est [app_prediction_gui.py](app_prediction_gui.py). Elle permet une interaction directe avec le mod√®le Random Forest entra√Æn√©.

### Fonctionnalit√©s Cl√©s :
-   **Interface Responsive** : Le design s'adapte automatiquement √† la taille de votre √©cran (grille dynamique).
-   **Synchronisation en Temps R√©el** : Les curseurs (Scales) et les zones de texte sont li√©s pour une saisie rapide et pr√©cise.
-   **Analyse d'Importance** : Lors de chaque clic sur "Pr√©dire", le logiciel isole les 3 facteurs qui ont le plus contribu√© au risque calcul√©.
-   **R√©sultats Scrollables** : Un rapport de risque d√©taill√© avec recommandations de s√©curit√©.

---

## üöÄ Installation et D√©marrage

### 1. Installation des d√©pendances
Assurez-vous d'avoir Python 3.8+ install√©, puis lancez :
```bash
pip install -r requirements.txt
```

### 2. Lancer l'Application
```bash
python app_prediction_gui.py
```

---

## üìÇ Organisation du Workspace

-   **Logiciel Principal** : [app_prediction_gui.py](app_prediction_gui.py) (Version finale √©pur√©e de commentaires).
-   **Documentation Fondamentale** : [documentation_code.md](documentation_code.md) (Explications exhaustives de toutes les notions ML et techniques).
-   **Validation Scientifique** : [validation_croisee.py](validation_croisee.py) et [validation_croisee_details.md](validation_croisee_details.md).
-   **Mod√®les Sauvegard√©s** : Le dossier `models/` contient les fichiers `.joblib` pour un chargement instantan√© sans r√©-entra√Ænement.

---

## üìú Licence & Cr√©dits
Ce projet a √©t√© r√©alis√© dans le cadre du cours **INF 365 - Intelligence Artificielle et Syst√®mes Complexes** par le **Groupe 24**.

- **Responsable IA** : [Votre Nom]
- **Ing√©nierie Logicielle** : [Votre Nom]
- **Dataset Analysis** : [Votre Nom]

---
*Fin du document README.*

```python
import pandas as pd
df = pd.read_csv("datas/flood.csv")
df.head()
df.info()
```

**Concepts cl√©s** :
- **DataFrame** : Structure de donn√©es tabulaire (lignes et colonnes)
- **Exploration** : Comprendre la structure, les types de donn√©es, et les statistiques descriptives

### 2Ô∏è‚É£ Nettoyage des Donn√©es

**Objectif** : Assurer la qualit√© des donn√©es pour un apprentissage efficace.

#### ‚úÖ V√©rification des valeurs manquantes
```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.show()
```
- Les valeurs manquantes peuvent biaiser le mod√®le.
- Dans notre cas, pas de valeurs manquantes d√©tect√©es.

#### ‚úÖ D√©tection des doublons
```python
print("Nombre de lignes dupliqu√©es:", df.duplicated().sum())
```
- Les doublons r√©duisent la diversit√© de l'apprentissage.

#### ‚úÖ Gestion des Outliers (Valeurs Aberrantes)

**Concept** : Un outlier est une valeur extr√™mement diff√©rente des autres.

**M√©thode IQR (Interquartile Range)** :
```python
for col in num_cols:
    Q1 = df[col].quantile(0.25)  # 1er quartile (25%)
    Q3 = df[col].quantile(0.75)  # 3e quartile (75%)
    IQR = Q3 - Q1                # √âcart interquartile
    lower = Q1 - 1.5 * IQR       # Limite inf√©rieure
    upper = Q3 + 1.5 * IQR       # Limite sup√©rieure
    
    # Plafonner les valeurs hors limites
    df[col] = df[col].clip(lower, upper)
```

**Pourquoi** :
- Les outliers peuvent avoir un impact disproportionn√© sur les mod√®les.
- Exemple : Une intensit√© de mousson √† 1000 (impossible) vs normale √† 10.

#### ‚úÖ Analyse de Corr√©lation
```python
import seaborn as sns
sns.heatmap(df.corr(), cmap="coolwarm", center=0)
plt.show()
```

**Concept** : La **corr√©lation** mesure la relation lin√©aire entre deux variables.
- **Corr√©lation positive** : Quand une augmente, l'autre augmente aussi.
- **Corr√©lation n√©gative** : Quand une augmente, l'autre diminue.
- **Corr√©lation = 0** : Pas de relation lin√©aire.

---

### 3Ô∏è‚É£ Pr√©paration des Donn√©es pour le Machine Learning

#### ‚úÖ S√©paration Train/Test
```python
from sklearn.model_selection import train_test_split

X = df.drop("FloodProbability", axis=1)  # Features (entr√©es)
y = df["FloodProbability"]               # Target (sortie)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

**Ratio 80/20** :
- **80%** des donn√©es pour **entra√Æner** le mod√®le.
- **20%** des donn√©es pour **tester** le mod√®le (donn√©es non vues pendant l'apprentissage).

**Pourquoi `random_state=42`** :
- Garantit la **reproductibilit√©** des r√©sultats.

#### ‚úÖ Normalisation (Scaling)
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**Concept** : La normalisation met toutes les variables √† la m√™me √©chelle.

**Formule** : $X_{scaled} = \frac{X - \mu}{\sigma}$
- $\mu$ = moyenne
- $\sigma$ = √©cart-type

**Pourquoi** :
- Certaines variables (ex: Population) peuvent avoir des valeurs beaucoup plus grandes que d'autres (ex: Intensit√©).
- Cela peut biaiser les mod√®les, particuli√®rement ceux bas√©s sur la distance (comme KNN) et les r√©seaux de neurones.

---

## ü§ñ Les Trois Mod√®les

### Mod√®le 1 : R√©gression Lin√©aire

**Fichier** : `modeleRegressionLineaire.ipynb`

**Concept** : 
Suppose une relation **lin√©aire** entre les features et la cible.

**Formule** : $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n$

**Avantages** :
‚úÖ Simple et rapide √† entra√Æner  
‚úÖ Facile √† interpr√©ter  
‚úÖ Bon baseline pour comparer d'autres mod√®les  

**Inconv√©nients** :
‚ùå Suppose une relation lin√©aire (souvent peu r√©aliste)  
‚ùå Sensible aux outliers  
‚ùå Performance faible sur les donn√©es complexes  

**Code cl√©** :
```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_pred = lr.predict(X_test_scaled)
```

**R√©sultats attendus** :
- Performance mod√©r√©e sur ce dataset complexe.
- Dispersion importante des pr√©dictions autour de la ligne parfaite.

---

### Mod√®le 2 : For√™t Al√©atoire (Random Forest)

**Fichier** : `modeleForetAleatoire.ipynb`

**Concept** : 
Ensemble de **100 arbres de d√©cision** qui votent ensemble pour la pr√©diction.

**Analogie** : 
Si vous demandez √† 100 experts independants leur opinion et prenez la moyenne, vous aurez une meilleure pr√©diction qu'un seul expert.

**Fonctionnement** :
1. Chaque arbre apprend une partie diff√©rente des donn√©es.
2. Chaque arbre fait une pr√©diction.
3. La pr√©diction finale = **moyenne** de toutes les pr√©dictions.

**Avantages** :
‚úÖ G√®re bien les **relations non-lin√©aires**  
‚úÖ Robuste contre les **overfitting** (surapprentissage)  
‚úÖ Pas besoin de normaliser les donn√©es  
‚úÖ Capture les interactions complexes entre variables  

**Inconv√©nients** :
‚ùå Plus lent √† entra√Æner que la r√©gression lin√©aire  
‚ùå Moins interpr√©table (bo√Æte noire)  
‚ùå Peut √™tre gourmand en m√©moire  

**Code cl√©** :
```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred = rf.predict(X_test_scaled)
```

**R√©sultats attendus** :
- Meilleure performance que la r√©gression lin√©aire.
- Points plus serr√©s autour de la ligne parfaite.
- MSE significativement plus bas.

---

### Mod√®le 3 : R√©seau de Neurones (Deep Learning)

**Fichier** : `modeleReseauNeuronesSequential.ipynb`

**Concept** : 
Un r√©seau de **neurones artificiels** organis√©s en couches, inspir√© par le cerveau humain.

**Architecture du mod√®le** :
```
Input (20 features) ‚Üí Dense(64, ReLU) ‚Üí Dense(32, ReLU) ‚Üí Dense(1, Linear) ‚Üí Output
```

**Explication** :
- **Input** : 20 caract√©ristiques environnementales
- **Dense(64, ReLU)** : Premi√®re couche cach√©e avec 64 neurones, activation ReLU
  - **ReLU** (Rectified Linear Unit) : Active le neurone si entr√©e > 0, sinon 0
  - Permet au r√©seau d'apprendre des relations non-lin√©aires
- **Dense(32, ReLU)** : Deuxi√®me couche cach√©e avec 32 neurones
- **Dense(1, Linear)** : Couche de sortie avec 1 neurone et activation lin√©aire
  - G√©n√®re une pr√©diction continue entre 0 et 1

**Entra√Ænement** :
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(64, activation='relu', input_shape=(20,)),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mse')
model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test))
```

**Concepts cl√©s** :
- **Optimizer (Adam)** : Algorithme pour ajuster les poids du r√©seau
- **Loss (MSE)** : Erreur quadratique moyenne - mesure l'√©cart entre pr√©dictions et r√©alit√©
- **Epochs** : Nombre de fois o√π on passe les donn√©es dans le r√©seau

**Avantages** :
‚úÖ Peut apprendre des **relations tr√®s complexes**  
‚úÖ Excellent potentiel de performance  
‚úÖ Flexible et adaptable  

**Inconv√©nients** :
‚ùå N√©cessite **beaucoup de donn√©es** (nous avons 50k ‚úÖ)  
‚ùå Requiert du **tuning** (ajustement des hyperparam√®tres)  
‚ùå Temps d'entra√Ænement plus long  
‚ùå Moins interpr√©table (bo√Æte noire)  
‚ùå Risque d'overfitting  

**R√©sultats attendus** :
- Meilleures performances globales.
- Points tr√®s proches de la ligne parfaite.
- MSE tr√®s faible.

---

## üìà Validation Crois√©e (Cross-Validation)

**Fichier** : `validation_croisee.py`

### Pourquoi la Validation Crois√©e ?

Imaginez que vous testez un mod√®le sur une seule partition de test. Vous pourriez avoir de la **malchance** :
- Cette partition contient des donn√©es faciles √† pr√©dire ‚Üí Performance gonfl√©e.
- Cette partition contient des donn√©es difficiles ‚Üí Performance sous-estim√©e.

**Solution** : Faire plusieurs tests avec diff√©rentes partitions et faire la moyenne.

### M√©thode K-Fold

```python
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    # Entra√Æner et tester le mod√®le
```

**Processus (5-Fold)** :

1. **Fold 1** : Test sur 20%, Entra√Ænement sur les 80% restants
2. **Fold 2** : Test sur les 20% suivants, Entra√Ænement sur les 80% restants
3. **Fold 3** : ...
4. **Fold 4** : ...
5. **Fold 5** : ...

**R√©sultat final** = **Moyenne** des 5 scores de performance

### M√©triques d'√âvaluation

#### 1Ô∏è‚É£ **Mean Squared Error (MSE)**
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2$$

- Mesure l'√©cart moyen (au carr√©) entre les pr√©dictions et la r√©alit√©.
- **Valeur basse = bon mod√®le**
- Les erreurs importantes sont p√©nalis√©es fortement (au carr√©)

#### 2Ô∏è‚É£ **R¬≤ Score (Coefficient de D√©termination)**
$$R^2 = 1 - \frac{\sum (y_{true} - y_{pred})^2}{\sum (y_{true} - \bar{y})^2}$$

- Mesure le pourcentage de variance expliqu√©e par le mod√®le.
- **Plage** : 0 √† 1 (et peut √™tre n√©gatif pour tr√®s mauvais mod√®les)
- **R¬≤ = 1** : Pr√©dictions parfaites
- **R¬≤ = 0** : Le mod√®le n'est pas meilleur qu'une pr√©diction moyenne
- **R¬≤ < 0** : Le mod√®le est pire qu'une simple moyenne

### R√©sultats Attendus de la Validation Crois√©e

```
Linear Regression:
  - MSE moyen : ~0.005
  - R¬≤ moyen : ~0.85

Random Forest:
  - MSE moyen : ~0.001
  - R¬≤ moyen : ~0.95

Neural Network:
  - MSE moyen : ~0.0005
  - R¬≤ moyen : ~0.98
```

---

## üöÄ Comment Ex√©cuter le Projet

### 1. Installation des d√©pendances
```bash
pip install -r requirements.txt
```

### 2. Ex√©cuter les mod√®les individuels (Notebooks Jupyter)
```bash
# Ouvrir les notebooks dans Jupyter
jupyter notebook modeleRegressionLineaire.ipynb
jupyter notebook modeleForetAleatoire.ipynb
jupyter notebook modeleReseauNeuronesSequential.ipynb
```

### 3. Ex√©cuter la validation crois√©e
```bash
python validation_croisee.py
```
G√©n√®re un fichier `cv_results.json` avec les scores de chaque mod√®le.

### 4. G√©n√©rer la pr√©sentation
```bash
python generate_ppt.py
```
Cr√©e `presentation_projet.pptx` avec les explications et emplacements pour les graphiques.

---

## üìÅ Structure du Projet

```
TPE INF 365 - Prevision des crues/
‚îú‚îÄ‚îÄ datas/
‚îÇ   ‚îî‚îÄ‚îÄ flood.csv                          # Dataset (50k exemples)
‚îú‚îÄ‚îÄ modeleRegressionLineaire.ipynb         # R√©gression lin√©aire
‚îú‚îÄ‚îÄ modeleForetAleatoire.ipynb             # For√™t al√©atoire
‚îú‚îÄ‚îÄ modeleReseauNeuronesSequential.ipynb   # R√©seau de neurones
‚îú‚îÄ‚îÄ validation_croisee.py                  # Script de validation crois√©e
‚îú‚îÄ‚îÄ generate_ppt.py                        # G√©n√©rateur de pr√©sentation
‚îú‚îÄ‚îÄ create_theme.py                        # Cr√©ateur de th√®me
‚îú‚îÄ‚îÄ extract_pdf_info.py                    # Extracteur d'infos PDF
‚îú‚îÄ‚îÄ requirements.txt                       # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                              # Ce fichier
‚îú‚îÄ‚îÄ presentation_projet.pptx               # Pr√©sentation finale
‚îî‚îÄ‚îÄ cv_results.json                        # R√©sultats de validation crois√©e
```

---

## üìö Concepts Cl√©s √† Retenir

| Concept | D√©finition |
|---------|-----------|
| **Feature** | Variable d'entr√©e (ex: intensit√© de mousson) |
| **Target** | Variable de sortie √† pr√©dire (FloodProbability) |
| **Outlier** | Valeur anormalement √©loign√©e des autres |
| **Normalisation** | Mise √† l'√©chelle des variables |
| **Overfitting** | Mod√®le qui m√©morise au lieu d'apprendre |
| **Validation Crois√©e** | Technique de test robuste avec plusieurs partitions |
| **MSE** | Erreur quadratique moyenne |
| **R¬≤** | Pourcentage de variance expliqu√©e |

---

## üéì Apprentissages du Projet

1. **Importance de la pr√©paration** : 80% du temps en data science va √† la pr√©paration des donn√©es
2. **Comparaison de mod√®les** : Pas d'algorithme unique meilleur pour tous les cas
3. **Validation robuste** : La validation crois√©e donne une estimation plus fiable de la performance
4. **Trade-offs** : Complexit√© vs Interpr√©tabilit√© vs Temps d'entra√Ænement
5. **Normalisation** : Cruciale pour les mod√®les bas√©s sur la distance et r√©seaux de neurones

---

## üë• Membres du Groupe 24

- **NITOPOP JEATSA GUILLAUME MELVIN** (CHEF) - 20S43003
- **DEMANOU KEMKENG DILAN** - 25S03516
- **NOSSI YIMGO LYNDSEY SULIVANE** - 23S87713
- **TCHIEUTCHOUA FOTEPING ASHLEY MEGANE** - 23S87863
- **WANGA POUYA KAVEN SAMIRA** - 23S88070

**Examinateur** : Dr Justin MOSKOLAI  
**Universit√©** : Universit√© de Douala - Facult√© des Sciences

---

## üìû Questions et Clarifications

Pour toute question sur le projet, consultez d'abord les notebooks individuels, puis les ressources en ligne sur scikit-learn, TensorFlow et pandas.

---

**Derni√®re modification** : January 2026
